%% ================ React ================ %%

@online{React,
  author = {Facebook},
  url = {https://reactjs.org},
  title = {{React} A JavaScript library for building user interfaces},
  date = 2014,
  urldate = {2019-01-10}
}

@video{ReactFoundingVideo,
 author={TXJS},
 title={Pete Hunt | TXJS 2015},
 howpublished={Youtube},
 year={2015},
 month={6},
 note={Pete Hunt talking about React and when it was founded},
 hyphenation={english},
 keywords={React},
 url={https://www.youtube.com/watch?v=A0Kj49z6WdM}
}

@video{ReactReconcliliationVideo,
 author={Facebook Developers},
 title={Lin Clark - A Cartoon Intro to Fiber - React Conf 2017},
 howpublished={Youtube},
 year={2017},
 month={3},
 note={Lin Clark talking about React and its fiber reconciliation algorithm},
 hyphenation={english},
 keywords={React},
 url={https://www.youtube.com/watch?v=ZCuYPiUIONs}
}

@online{ReactRenderCycleDiagram,
  author={Wojciech Maj},
  url={http://projects.wojtekmaj.pl/react-lifecycle-methods-diagram/},
  title={{React} lifecycle methods diagram},
  urldate={2019-05-10},
} 

@online{ReactCycleTweet,
  author={Dan Abramov},
  url={https://twitter.com/dan_abramov/status/981712092611989509},
  title={{Twitter Post:} I just made this diagram of modern React lifecycle methods. Hope you’ll find it helpful!},
  urldate={2019-05-10},
}

@software{ReactMove,
  author={Steve Hall},
  url={https://github.com/react-tools/react-move},
  title={{React Move} Beautiful, data-driven animations for React},
  urldate={2019-05-29},
  version={5.2.1}
}

@software{ReactStorybook,
  author={storybookjs},
  url={https://github.com/storybookjs/storybook},
  title={UI component dev \& test: React, Vue, Angular, React Native, Ember, Web Components \& more!},
  urldate={2019-06-03},
  version={5.1.0}
}

@software{ReactRenderCycleGithub,
  author={Wojciech Maj},
  url={https://github.com/wojtekmaj/react-lifecycle-methods-diagram},
  title={{React} Lifecycle Methods diagram},
  urldate={2019-05-10},
  version={1.0.0}
}

%% ================ D3 ================ %%

@software{D3Github,
  author = {Mike Bostock},
  url = {https://github.com/d3},
  title = {{D3} Data Driven Documents},
  date = {2011},
  urldate = {2019-01-10},
  version = {5.9.2}
}

@online{D3Website,
  author = {Mike Bostock},
  url = {https://d3js.org},
  title = {{D3} Data Driven Documents},
  date = {2011},
  urldate = {2019-01-10},
  version = {5.9.2}
}

@online{D3Examples,
  author = {Mike Bostock},
  url = {https://bl.ocks.org/mbostock},
  title = {Mike Bostock's Blocks},
  urldate = {2019-01-10},
}

%% ================ Misc ================ %%

@software{UberVisForce,
  author = {Uber},
  version = {0.3.1},
  url = {https://github.com/uber/react-vis-force},
  title = {{UberVisForce} d3-force graphs as React Components},
  urldate = {2019-01-10}
}

@online{RAF,
  author = {Mozilla Foundation},
  url = {https://developer.mozilla.org/en-US/docs/Web/API/window/requestAnimationFrame},
  title = {{Mozilla} Request Animation Frame},
  urldate = {2019-01-25}
}

@online{ChromeRAF,
  author = {Google},
  url = {https://developers.google.com/web/updates/2012/05/requestAnimationFrame-API-now-with-sub-millisecond-precision},
  title = {{Chrome} Request Animation Frame},
  urldate = {2019-01-25}
}

@software{ImmutableJS,
  author = {Lee Byron},
  url = {https://github.com/immutable-js/immutable-js},
  title = {{Immutable} Immutable collections for JavaScript},
  urldate = {2019-05-02},
  version = {3.8.2}
}

@software{Babel,
  author = {Babel},
  url = {https://babeljs.io/},
  title = {{Babel} Babel is a JavaScript compiler},
  urldate = {2019-05-02},
  version = {7.4.0}
}

@software{SeedRandom,
  author={David Bau},
  url={https://github.com/davidbau/seedrandom},
  title={seeded random number generator for Javascript},
  urldate={2019-05-30},
  version={3.0.1}
}

@online{StatCounterBrowserMarketShare,
  author = {StatCounter},
  url = {http://gs.statcounter.com/},
  title = {{statcounter} Browser Market Share Worldwide},
  urldate = {2019-06-07}
}

@online{W3CBrowserMarketShare,
  author = {W3C},
  url = {https://www.w3counter.com/globalstats.php},
  title = {{W3C} Browser \& Platform Market Share},
  urldate = {2019-06-07}
}

%% ================ Theses ================ %%

@inproceedings{lloyd1994practical,
  title = {Practical advantages of declarative programming},
  author = {JW. Lloyd},
  note = {Conference Proceedings/Title of Journal: Joint Conference on Declarative Programming},
  year = {1994},
  pages = {3--17},
}

%% ================ Books ================ %%

@BOOK{prgLngDesignImpl,
	AUTHOR = {Pratt, Terrence W. AND Zelkowitz, Marvin V.},
	YEAR = {2001},
	TITLE = {Programming Languages - Design and Implementation},
	EDITION = {4},
	ISBN = {978-0-130-27678-0},
	PUBLISHER = {Prentice Hall},
	ADDRESS = {London},
}

@book{RestorationOfMotionPictureFilm,
	AUTHOR = {Read, Paul AND Meyer, Mark-Paul},
	YEAR = {2000},
	TITLE = {Restoration of Motion Picture Film - },
	EDITION = {1},
	ISBN = {978-0-080-51619-6},
	PUBLISHER = {Elsevier},
	ADDRESS = {Amsterdam},
}

@Article{Potter2014,
  author={Potter, Mary C.
  and Wyble, Brad
  and Hagmann, Carl Erick
  and McCourt, Emily S.},
  title={Detecting meaning in RSVP at 13 ms per picture},
  journal={Attention, Perception, \& Psychophysics},
  year={2014},
  month={2},
  day={1},
  volume={76},
  number={2},
  pages={270--279},
  abstract={The visual system is exquisitely adapted to the task of extracting conceptual information from visual input with every new eye fixation, three or four times a second. Here we assess the minimum viewing time needed for visual comprehension, using rapid serial visual presentation (RSVP) of a series of six or 12 pictures presented at between 13 and 80 ms per picture, with no interstimulus interval. Participants were to detect a picture specified by a name (e.g., smiling couple) that was given just before or immediately after the sequence. Detection improved with increasing duration and was better when the name was presented before the sequence, but performance was significantly above chance at all durations, whether the target was named before or only after the sequence. The results are consistent with feedforward models, in which an initial wave of neural activity through the ventral stream is sufficient to allow identification of a complex visual stimulus in a single forward pass. Although we discuss other explanations, the results suggest that neither reentrant processing from higher to lower levels nor advance information about the stimulus is necessary for the conscious detection of rapidly presented, complex visual information.},
  issn={1943-393X},
  doi={10.3758/s13414-013-0605-z},
}

@ARTICLE{6375944,
  author={X. {Wu} and G. {Zhai}},
  journal={IEEE Signal Processing Magazine},
  title={Temporal Psychovisual Modulation: A New Paradigm of Information Display [Exploratory DSP]},
  year={2013},
  volume={30},
  number={1},
  pages={136-141},
  keywords={display instrumentation;optoelectronic devices;temporal psychovisual modulation;information display;human civilization;rock carving;prehistoric tribesmen;optoelectronic displays;signal processing;psychophysics;Information systems;Optoelectronic devices;Modulation;Physics;Signal processing;Headphones;Psychology;Behavioral science;Displays},
  doi={10.1109/MSP.2012.2219678},
  ISSN={1053-5888},
  month={1},
} 

@inproceedings{farrell1987predicting,
  title={Predicting flicker thresholds for video display terminals},
  author={Farrell, J. E. and Benson, Brian L. and Haynie, Carl R.},
  booktitle={Proceedings of the System Insight Display},
  volume={28},
  pages={449--453},
  year={1987},
}

@article{stereoscopicFlickerArticle,
  author = {Hoffman, David M. and Karasev, Vasiliy I. and Banks, Martin S.},
  title = {Temporal presentation protocols in stereoscopic displays: Flicker visibility, perceived motion, and perceived depth},
  journal = {Journal of the Society for Information Display},
  volume = {19},
  number = {3},
  pages = {271-297},
  keywords = {Stereoscopic 3-D, stereo, spatio-temporal sampling, triple flash, frame-rate conversion, judder, flicker, depth distortion, field sequential, time multiplexed, shutter glasses},
  abstract = {Abstract— Most stereoscopic displays rely on field-sequential presentation to present different images to the left and right eyes. With sequential presentation, images are delivered to each eye in alternation with dark intervals, and each eye receives its images in counter phase with the other eye. This type of presentation can exacerbate image artifacts including flicker, and the appearance of unsmooth motion. To address the flicker problem, some methods repeat images multiple times before updating to new ones. This greatly reduces flicker visibility, but makes motion appear less smooth. This paper describes an investigation of how different presentation methods affect the visibility of flicker, motion artifacts, and distortions in perceived depth. It begins with an examination of these methods in the spatio-temporal frequency domain. From this examination, it describes a series of predictions for how presentation rate, object speed, simultaneity of image delivery to the two eyes, and other properties ought to affect flicker, motion artifacts, and depth distortions, and reports a series of experiments that tested these predictions. The results confirmed essentially all of the predictions. The paper concludes with a summary and series of recommendations for the best approach to minimize these undesirable effects.},
  year = {2011}
}