\chapter{Performance Testing and user perception}
\label{cha:performance}

The research question of the thesis is, if React can be combined with D3 without losing any performance in the browser. The following sections intoduce the reader not only to the benchmark setup and testing methodologies, but also to the final results of the benchmarks. There is also a discussion which elaborates all test results. Since humans cannot measure the exact amount of frames per second, a special section introduces the reader to the human perception of fluid animations and how the testing results can be interpreted even further with that knowledge.

\section{Test environment setup}

This section describes the testing environment that was implemented to compare the three force simulation component prototypes. There are a few challenges that had to be taken into account when realizing the testing environment. Also, the implementation details are elaborated and explained. Last but not least, the testing devices are introduced which are used to run the benchmark.

\subsection{Challenges}

One of the most challenging aspects of the thesis project is the performance measurement of the prototypes. Modern browsers have an uncountable amount of features that help to smooth out the performance to improve user perception. Getting some consistent performance numbers is much harder due to inconsistent browser optimizations. Using different browsers for benchmarks also means that different JavaScript runtimes are used to run the benchmarks. All engines have different execution and parsing speeds. Also, the mechanism to speed up frequently accessed script code is different, which also makes it harder to get consistent performance numbers.

When benchmarking web applications, it is very complicated to get performance values that have scientific relevance, which can be compared to get some accurate research results. The next section introduces a system that was primarily implemented to measure the performance of JavaScript web applications. The system tries to tackle all challenges to produce detailed benchmark results.

Another big problem is the fact that browsers detect the refresh rate of monitors when utilizing the request animation frame functionality. Monitors with a refresh rate of 144 hertz allow browsers to produce up to 144 frames per second. 60-hertz monitors, on the other hand, limit the browser's framerate to 60 FPS. The maximum amount of animation frame executions in any browser can only ever go up to the monitor's amount of hertz the browser window is running in. 

Speaking of utilizing the request animation frame functionality, it must be mentioned that each browser has a different implementation of the functionality. The Chrome browser, for example, is smoothing out the performance by trying to executing animation frames regularly, which means that the overall performance might be lowered to achieve a smoother framerate. Experience shows that Firefox, on the other hand, always fires its animation frames whenever a frame is available, which can result in higher overall but not as consistent framerates.

Last but not least, another challenge was to build a performance measuring environment that can be used without adding code to the prototypes. Theoretically, each prototype should be a complete component that can be shipped as a third party library. By adding performance measurement specific code, the components would contain functionality that is not needed when shipping production builds of the components.

\begin{figure}
  \centering
  \includegraphics[width=1\columnwidth]{testresults}
  \caption{Hybrid prototype benchmark results}
  \label{fig:reactD3benchResult}
\end{figure}

\subsection{Building a stable Testing Environment}
\label{sub:perfImplDetails}

A testing environment which produces consistent data input across multiple iterations yields the best testing results when benchmarking all three prototypes. Testing the force simulation prototypes with the same data across multiple benchmarks, environments, and devices is of utmost importance. A valid solution is to use a pseudo-random data generator which can be restarted and reseeded each benchmark iteration.

Generating an arbitrary amount of random node and link positions is not a problem, as JavaScript has a built-in random generator which can be utilized to generate random data. Unfortunately, JavaScript's random generator cannot be seeded to achieve consistent pseudo-random results. The library \emph{seedrandom} in \cite{SeedRandom} is the perfect technology to solve the problem of generating consistent data across multiple iterations of the benchmark tests.

Since browsers frequently yield different performance numbers across iterations with identical data, the amount cycle per test data iterations sets has to be increased. When testing one specific iteration multiple times, the average value has much more significance than testing an iteration only once. Therefore the testing environment must have support for different iteration configurations, which can be run an arbitrary amount of times.

\begin{figure}
  \centering
  \includegraphics[width=0.5\columnwidth]{reactd3bench}
  \caption{Overview of the benchmark application}
  \label{fig:reactD3bench1}
\end{figure}

The testing environment is designed to be a stateful container which generates some random data and then passes it to the desired prototype. D3 simulations provide a mechanism to add an event handler whenever the simulation stops. It is, therefore, no problem to pass a handler to the benchmark component that is executed whenever the simulation stops. The handler can be used to restart the benchmark with some newly generated data until the desired amount of benchmark iteration cycles is reached.

Pseudo-random generated data is obtained by a specially implemented custom helper utility module. The custom module is designed to generate a specified amount of random nodes and links. The method to generate random data takes two parameters -- the number of nodes and the number of links. Generating consistent pseudo-random data is possible by internally using the previously mentioned seedrandom module.

Tackling the performance measurement problem is a much harder task, however. To be able to measure the number of frames per second, the request animation browser functionality can be used. Another custom implemented module provides some functionality that is specifically designed to measure performance intensive JavaScript animations. By requesting an animation frame as often as possible in a terminable infinite loop, a reference timestamp can be used to measure the amount of animation frame executions per seconds which is equal to the frames per second the browser produces.

Presenting the performance results mustn't be underestimated either. To provide benchmark results appealingly, each iteration is represented via a visual container that contains all relevant information for a specific test iteration. If there are 6 test iterations, 6 containers are rendered after the benchmark. The containers contain the number of cycle per iteration, the test configuration, the number of frames per second, the overall execution time per cycle, the average frame time, and also the highest frame time. Figure \ref{fig:reactD3benchResult} shows how the benchmark results visually look like.

Finally, the benchmark tool has to be easy to use on all kinds of devices. The thesis project also contains a small React application which can be deployed on any static web hosting service that can serve single page applications. A visual representation of the basic benchmark app can be seen in figure \ref{fig:reactD3bench1}. The React application is a wrapper around the testing environment, which lets users select the desired benchmark for any of the three force simulation prototypes and then runs it in the browser. Each benchmark also has an easily distinguishable URL to be able to copy paste a specific benchmark URL into any browser. Figures \ref{fig:reactD3bench1} and \ref{fig:reactD3bench2} show how the URL contains all relevant benchmark parameters. That way the benchmark URLs can be pasted into any browser to execute the benchmark. 

\begin{figure}
  \centering
  \includegraphics[width=0.6\columnwidth]{reactd3bench1}
  \caption{The benchmark currently iterating through hybrid prototype iterations}
  \label{fig:reactD3bench2}
\end{figure}

\section{Testing setup}

Having a bulletproof testing setup plays a fundamental role in producing scientifically releveant test results. Therefore, the following sections of the thesis provide a thorough insight on the testing devices and on how the whole benchmark methodology is conceptionalized. 

\subsection{Testing devices}

\begin{table}
  \centering
  \begin{threeparttable}
    \caption{The table shows a list of low-end testing devices.}
    \label{tab:lowendTestingDevices}
    \centering
    \def\rr{\rightskip=0pt plus1em \spaceskip=.3333em \xspaceskip=.5em\relax}
    \setlength{\tabcolsep}{1ex}
    \def\arraystretch{1.20}
    \setlength{\tabcolsep}{1ex}
    \small
    \begin{english}
      \begin{tabular}{|c||c|c|c|}
        \hline
          \multicolumn{1}{|c||}{\emph{Device}}&
          \multicolumn{1}{|c}{\emph{CPU}} &
          \multicolumn{1}{|c}{\emph{GPU}} &
          \multicolumn{1}{|c|}{\emph{RAM}} \\
        \hline
        \hline
        OnePlus 1 & 
        Snapdragon 801 & 
        Adreno 330 & 
        3GB \\
        \hline
        OnePlus 5T & 
        Snapdragon 835 & 
        Adreno 540  & 
        8GB \\
        %%\hline
        %%Samsung G\tnote{1} \hspace{0.1mm} S8 & 
        %%Exynos 8895 & 
        %%Mali-G71 MP20 & 
        %%4GB \\
        %%\hline
        %%Samsung GT\tnote{2} \hspace{0.1mm} A10.1 &
        %%Exynos 7870 & 
        %%Mali-T830 MP2 & 
        %%3GB \\
        \hline
        SurfaceBook & 
        Intel i5-6300U & 
        Intel HD 520 & 
        8GB \\
        \hline
      \end{tabular}  
    \end{english}
    %%\begin{tablenotes}
    %%\item [1] Galaxy
    %%\item [2] Galaxy Tab
    %%\end{tablenotes}
  \end{threeparttable}
\end{table}

The amount of test devices should be as high as possible while still being reasonable regarding to the effort it takes to process all resulting benchmark data. A total amount of 6 devices is enough to retrieve scientifically significant results. The range of devices is divided into 2 sections: high-end devices and low-end devices. 

The mobile devices used for testing are a OnePlus 1 phone, a OnePlus 5t phone, and a SurfaceBook in tablet mode. All low-end devices and their specs are listed in table \ref{tab:lowendTestingDevices}. It must be noted, that every selected low-end device has a monitor refresh rate of 60-hertz. 

The table \ref{tab:highendTestingDevices} introduces all high-end devices which have a monitor refresh rate of 144-hertz. Two of the listed devices are custom tower builds with custom specs and one device is a the Razer Blade 15 laptiop with specs defined by its manufacturer. All in all the devices should provide a good overview of the performance of the force graph components.

\begin{table}
  \centering
  \begin{threeparttable}
    \caption{The table shows a list of high-end high refresh rate testing devices.}
    \label{tab:highendTestingDevices}
    \centering
    \def\rr{\rightskip=0pt plus1em \spaceskip=.3333em \xspaceskip=.5em\relax}
    \setlength{\tabcolsep}{1ex}
    \def\arraystretch{1.20}
    \setlength{\tabcolsep}{1ex}
    \small
    \begin{english}
      \begin{tabular}{|c||c|c|c|c|}
        \hline
          \multicolumn{1}{|c||}{\emph{Device}}&
          \multicolumn{1}{|c}{\emph{CPU}} &
          \multicolumn{1}{|c}{\emph{GPU}} &
          \multicolumn{1}{|c|}{\emph{RAM}} \\
        \hline
        \hline
        Tower (Max Z.) & 
        Intel i9-7900X & 
        2x Nvidia GTX 1080Ti & 
        32GB \\
        \hline
        Razer Blade 15 (2018) & 
        Intel i7-8750H & 
        Nvidia GTX 1070 Max-Q  & 
        16GB \\
        %%\hline
        %%Tower (Max J.) & 
        %%Intel i7-7700k & 
        %%Nvidia GTX 1070 & 
        %%16GB \\
        \hline
        Tower (Patrick M.) &
        Intel i7-6700k & 
        Nvidia GTX 1080 & 
        16GB \\
        %%\hline
        %%Tower (Julian J.) & 
        %%Intel i7-7700k & 
        %%Nvidia GTX 1070 & 
        %%16GB \\
        \hline
      \end{tabular}  
    \end{english}
  \end{threeparttable}
\end{table}

\subsection{Testing methodologies}

\begin{figure}
\centering
\includegraphics[scale=2.5, trim= 4cm 5cm 4cm 5cm, clip, width=1\columnwidth]{perfIterations.pdf}
\caption{Benchmark iteration configuration}
\label{fig:perfIterations}
\end{figure}

Running the benchmarks to get valid and scientific testing results is relatively straight forward. To get the most declarative performance numbers it is best to mainly use devices with a high monitor refresh rate. Devices with a low monitor refresh rate can possibly falsify some testing results. If for example a browser could possibly render 100 frames per second on an iteration, a system with a 60-hertz monitor would only be able to measure a maximum of 60FPS as the theoretically possible 100 FPS would be capped to the monitor's 60-hertz refresh rate.

Devices with high refresh rates can be sufficient for measuring the overall best performing prototype. One of the more interesting research aspects of the thesis though is the question, how well the prototypes perform on mobile devices with lower performance specifications than desktop PCs. Thus the testing results must be split up into different categories as a consequence. Due to the fact that not only frames per second, but also other perfromance aspects like total execution time are measured, the lower performing devices can also be compared to each other.

Each device runs through 6 iterations per prototype with exponentially increasing render difficulty. Figure \ref{fig:perfIterations} shows the benchmark configuration for the benchmark. Starting with a node count of 10 and a link count of five, the configuration ultimately goes up to 1000 nodes and 500 links. The third iteration is special, as the number of nodes and links is the same. The special configuration was added to test an extreme scenario of all nodes being connected to each other to induce some extra performance heavy force calculations.

Each iteration runs through 10 cycles which equals a grand total of 60 cycles per prototype, browser, and device combination. Three high-end devices with 144-hertz monitors and the three low-end devices with 60-hertz monitors run the benchmark iterations in the Chrome and in the Firefox browser. The two browsers were selected, because they're the most significantly used, platform independent brwosers worldwide according to the statistics in \cite{StatCounterBrowserMarketShare} and \cite{W3CBrowserMarketShare}. Taking into account that there are 6 devices, 3 prototypes, and 2 browsers, the total number of iterations is 36 and the total number of iteration cycles is 360 as every iteration has 10 cycles.

\section{Benchmark Results}

This section answers the research question if React can be combined with React without introducing any performance losses in the browser. Extensive benchmark testing sessions resulted in some interesting research results which are presented below. After presenting the test results, an introduction to the human perception of fluid animations helps the reader to follow the subsequent interpretation of the benchmark results.

\subsection{Introducing the test results}

First off all, the thesis project is a success, as the overall performance numbers show a clear trend that the hybrid prototype is ahead of Uber's pure react implementation. The total execution time of all combined benchmark iteration cycles is exactly 25326ms when combining the average execution times of each test. When converting milliseconds to hours the result is 7.04 hours. Since the benchmark environment was designed for extensive benchmark sessions, the time between running the benchmarks was minimized. The only tedious and most actively time consuming task was to write down the actual benchmark results. Letting devices run the tests required no further user interaction. 

Starting off with the low-end devices, figure \ref{fig:perfLowEnd001} shows the average FPS of the low end devices. An overall downwards trend can immediately seen in the FPS chart which is expected of course as the more DOM nodes the browsers have to calculate, the lower the FPS get per iteration. Each group of bars in the chart represents an average value for each prototype per benchmark iteration. Note, that all values are the average taken from 10 iteration cycles of the chrome and the firefox browser. The prototypes mostly yielded the expected performance numbers starting with the reference performance of the pure D3 protype, followed by the hybrid component and then finally followed by the pure React force graph component.

\begin{figure}
\centering
\includegraphics[scale=2.5, trim= 4cm 4.5cm 4cm 4.5cm, clip, width=1\columnwidth]{perfLowEnd001.pdf}
\caption{Low-end devices' average frames per second per benchmark iteration cycle (higher is better)}
\label{fig:perfLowEnd001}
\end{figure}

Following up with the high-end devices, figure \ref{fig:perfHighEnd001} shows the average FPS values of the high-end benchmark iterations. Looking at the bar chart, it is apparent that the high end devices hit the fps cap of 144FPS throughout the first few iteration cycles. As mentioned before, the devices would have been able to put out a higher number of frames per second but were limited to the browsers animation frame functionality which is in itself limited to the devices' monitor refresh rate. Beginning from the third iteration, a steady decrease of FPS can observed though, as the iteration difficulty is high enough for all devices to not hit the monitor refresh rate limit anymore.

\begin{figure}
\centering
\includegraphics[scale=2.5, trim= 4cm 4.5cm 4cm 4.5cm, clip, width=1\columnwidth]{perfHighEnd001.pdf}
\caption{High-end devices' average frames per second per benchmark iteration cycle (higher is better)}
\label{fig:perfHighEnd001}
\end{figure}

The bar chart in \ref{fig:perfLowEnd002} shows the average time it took to fully complete the benchmark. Via the browsers' performance API exact timestamps can be measured once a benchmark cycle starts and once it ends. By subtracting the start timestamp from the end timestamp the overall time to execute is calculated. Like the FPS chart, the time to complete chart also shows all average values it took the different prototypes to fully complete the benchmark cycles.

\begin{figure}
\centering
\includegraphics[scale=2.5, trim= 4cm 4.5cm 4cm 4.5cm, clip, width=1\columnwidth]{perfLowEnd002.pdf}
\caption{Low-end devices' average time to complete for one benchmark iteration cycle in milliseconds (lower is better)}
\label{fig:perfLowEnd002}
\end{figure}

Continuing with the high-end devices, the chart in \ref{fig:perfHighEnd002} shows the average time in milliseconds it took the devices to finish one iteration cycle. The results show, that also the time to complete is capped at a minimum value throughout the first iteration cycles. Since the animation frame performance measurement utility is bound to the browsers' animation frame functionality, being capped at a maximum value also means being restricted on minimum values of course.

\begin{figure}
\centering
\includegraphics[scale=2.5, trim= 4cm 4.5cm 4cm 4.5cm, clip, width=1\columnwidth]{perfHighEnd002.pdf}
\caption{High-end devices' average time to complete for one benchmark iteration cycle in milliseconds (lower is better)}
\label{fig:perfHighEnd002}
\end{figure}

%% frametime another way to display frames per second, just another calculation

Measuring the average frame time of animations can provide insights to the user's perception of a fluid animation. If the value is too high, the animation may not be experienced as a smooth animation. Therefore measuring the value is crutial when comparing the prototypes to each other. Figure \ref{fig:perfLowEnd003} shows a barchart of the low-end device benchmark. Towards the last 2 iterations the benchmark configuration seems to have hit a certain threshold since the values rise exponentially. The other performance show a similar pattern to the previous results though.

\begin{figure}
\centering
\includegraphics[scale=2.5, trim= 4cm 4.5cm 4cm 4.5cm, clip, width=1\columnwidth]{perfLowEnd003.pdf}
\caption{Low-end devices' average frame time per benchmark iteration cycle (lower is better)}
\label{fig:perfLowEnd003}
\end{figure}

Looking at the high end results in \ref{fig:perfHighEnd003} the same problem as before can be observed, where the first iterations are capped to a specific minimum value. The rest of the results increase exponentially though which corelates to the rest of the high-end performance results. The average frame time of the last iteration breaks out and spikes with an exceptionally high value.

\begin{figure}
\centering
\includegraphics[scale=2.5, trim= 4cm 4.5cm 4cm 4.5cm, clip, width=1\columnwidth]{perfHighEnd003.pdf}
\caption{High-end devices' average frame time per benchmark iteration cycle (lower is better)}
\label{fig:perfHighEnd003}
\end{figure}

Last but not least a critical aspect of any animation performance measurement is the maximum time between frames measured. The value can provide critical insights to some performance issues even tough the average frame time per second might look fine. The chart in \ref{fig:perfLowEnd004} shows an average of the maximum frame time value to each iteration cycle. One unanticipated result was, that the maximum frame rate of the hybrid component is significantly higher throughout the testing results than the pure D3 or pure React components.

\begin{figure}
\centering
\includegraphics[scale=2.5, trim= 4cm 4.5cm 4cm 4.5cm, clip, width=1\columnwidth]{perfLowEnd004.pdf}
\caption{Low-end devices' average maximum frame time per benchmark iteration cycle (lower is better)}
\label{fig:perfLowEnd004}
\end{figure}

However, the performance graph in \ref{fig:perfHighEnd004} shows more expected results. Again, the average maximum frame time rises exponentially. The results contain a few irregularities though. The maximum frame time of the reference pure D3 prototype for example is sometimes higher as the value of the other two prototypes. The maximum frame time values therefore must be taken with a grain of salt, as any maximum value could be falsified by unexpected system or browser activities which could have decreased the overall performance of any prototype. As mentioned before, the problem is mitigated by executing one iteration multiple times, but there is still a margin of error though.

\begin{figure}
\centering
\includegraphics[scale=2.5, trim= 4cm 4.5cm 4cm 4.5cm, clip, width=1\columnwidth]{perfHighEnd004.pdf}
\caption{High-end devices' average maximum frame time per benchmark iteration cycle (lower is better)}
\label{fig:perfHighEnd004}
\end{figure}

\subsection{Human Perception of fluid animations}
\label{sub:humanPerception}

%%Humans vision is bad lol, they can only see 13 fps but when do we actually notice some stuttering or laggy animation performance? 

Human vision is a very complicated topic, as there has been a large volume of studies for years which tried to determine at which point humans percieve a series of images as fluid motion. The article in \cite{RestorationOfMotionPictureFilm} claims, that humans perceive motion if the animation is displayed with at least 12 frames per second. During further research another article was found which states, that humans can detect specific images in a rapid serial visual representation (RSVP) of a series of multiple pictures. The paper in \cite{Potter2014} found, that participants could determine if the RSVP stream contained a specific picture even with a frequency of displaying each picture for just 13ms. 

Another vital question is at which point an observer doesn't percieve an animation as fluid anymore. As mentioned before, the time per frame should be below 13ms in the best case to to provide the perception of a fluid animation. Although animations with at least 12 frames per second can be percieved as motion, they are not necessarily experienced as fluid motion though. As the performance measurements of the thesis project yields definitive numbers, they can be used to determine if the animation of the test can be percieved as fluid motion or not.

Further research in the field of human perception of animation revealed yet another interesting result. Countless studies  throughout the last decades have tried to find an answer to the question at which point humans do not experience an animation as stutterin or flickering anymore. The answer of most research papers is though, that flickering or a so called "lag" cannot be detected, if the human perception cannot distinguish between modulated light and a stable field anymore. After doing some extensive research, the rate seems to be between 50 and 90 hertz according to serveral papers like \cite{6375944}, \cite{farrell1987predicting} or \cite{stereoscopicFlickerArticle}. The papers are mostly about the monitors refresh rate, but the same also applies for the actual displayed frames per second of an animation rendered in the browser.

\subsection{Interpreting the test results}

When interpreting the test results, it is important to keep in mind that the hybrid implementation uses a custom animation library internally as mentioned in \ref{sub:D3AndReactHybrid} which can be turned off to improve performance. The animation feature was turned on during the execution of the benchmark tests even though the test iterations do not include any individual node animation. As a result, the pure React component has a clear advantage over the hybrid compoent by not having to calculate node animations as they're not implemented on the pure React component. Even though the additional performance decreasing feature was kept on during all tests, the hybrid component generally yielded equal or better performance numbers in comparison to the pure React prototype.

Another clear advantage of the React prototype is that newer versions of React provide a functionality which assigns lower priority to DOM nodes which are outside the viewport of the browser. Lin Clark explains the functionality of the library during the React conference in in \cite{ReactReconcliliationVideo}, where the feature was first introduced. Due to the fact, that mobile devices mostly have smaller viewports, a large proportion of the nodes in higher iteration difficulties is rendered outside of the viewport again providing an advantage to the React prototype.

Looking at the frames per seconds, the charts in \ref{fig:perfLowEnd001} and \ref{fig:perfHighEnd002} indicate how the D3 prototype yields the best result in every iteration, followed by the hybrid implementation with the second best results and lastly the pure React prototype with the slowest results. Closer inspection of the two charts shows, that the performance decrease is linear whereas the increase of render difficulty in \ref{fig:perfIterations} is exponential. Due to the fact that the frames per second are calculated via the \texttt{requestAnimationFrame()} function, the results show, that the browsers regularly try to provide animation frames even though the caluclations get exponentially harder.

In comparison to the earlier presented results, the overall time to complete values in the charts in \ref{fig:perfLowEnd002} and \ref{fig:perfHighEnd002} do not correlate to the frames per second which is a rather unexpected result. Like mentioned before, the browser tries to provide as many animation frames as possible whereas the overall time to complete is measured via two timestamps. A remarkable outcome though is the fact, that the first and most lightweight iteration results show, that the low-end devices' time to complete is roughly 40\% of the high-end devices' time to complete. This supports the theory, that the monitor refresh rate is directly tied to the browsers' request animation frame functionality as 40\% of 144 is roughly equals 60. 

The high-end and low-end time to complete values do not correlate either. While the high-end benchmark values in \ref{fig:perfHighEnd002} show the expected exponential increase of time to complete, the low-end results in \ref{fig:perfLowEnd002} show a linear increase for the time to complete one iteration cycle. The findings can be explained that browsers cannot complete full render and paint cycles anymore during one animation frame due to the low-end hardware. By generating a constand backlog of due animation frames, some of them might get dropped due to new animation frame requests that are more recent. Once animation frames get cancelled because of more recently requested animation frames, the pattern of increasing completion time is more comparable to the pattern the overall FPS increase in the charts \ref{fig:perfLowEnd001} and \ref{fig:perfHighEnd002}. The high-end results in \ref{fig:perfHighEnd002} show the expected exponential increase of time to complete an iteration cycle, as browsers can process the calculations completely whithin the requested animation frame.

Figures \ref{fig:perfLowEnd003} and \ref{fig:perfHighEnd003} show the average frame times of the protoypes. The results can be seen as another way to describe frames per second basically. As described in section \ref{sub:humanPerception}, the frame times play a crutial role for humans to percept an animation as fluid without stuttering. Near all low-end device benchmarks showed rather high average frametimes starting from the second benchmark iteration. While the results stayed well above the previously mentioned 12 frames per second threshold, the lower frame time is definitely noticable. Looking at the general trend again, the hybrid prototype generally comes out ahead, when comparing the average frame times. The high-end chart in \ref{fig:perfHighEnd003} shows a clear spike of the pure React implementation in the last iteration. A possible explanation for this outcome might be that React reaches the previously already mentioned animation frame calculation frame threshold as well. React performs pretty well considering it has to completely process 1500 DOM nodes every 30ms and put them through a complete render cycle.

Last but not least the maximum frame time results can also provide valuable insights to how the prototypes perform compared to each other. These results must be interpreted with caution though because maximum frame time spikes can be caused by various unforeseeable reasons. The results for the low-end devices in \ref{fig:perfLowEnd004} for example show, that the hybrid component had the highest maximum frame time in the last two iterations. Now, if this only happens once in the animation, this might not be noticable, but if this would be a continuous trend, the data could point to a performance problem in the implementation of the prototype. The explanation to the performance spikes is most likely also tied to the fact, that low-end devices run into the animation frame limit of not being able to calculate a full render cycle in one animation frame. The hybrid component not only has to build up the whole component tree via react but also calculate the animations with react move. If both instances are limited to animation frame contraints, the calculation could possibly be spread out across multiple frames, making the whole animation slower in the process.

On the contrary, the high-end maximum frame time tests in \ref{fig:perfHighEnd004} yield more expected results. When having to deal with countless nodes and links, the maximum frame times should be equally affected as a consequence. The last iteration shows the longest time between frames for the pure React prototype, which is 76 milliseconds. The pure React and hybrid component seem to be able to equally maintain a maximum frame time at about 60 milliseconds.

%Generally a correlation between the time to complete, average frame time and the maximum frame time can be observed when looking at all the charts again.

\section{Conclusion}

All in all the different prototypes performed quite well in general. The question "Why does the benchmark use as high numbers as 1000 nodes and 500 links?" which might come up during the examination of the testing results can easily be answered. The test that was used to benchmark the prototypes uses a simple force simulation configuration where each base node is just a single circle element and each link is a path element. When using more sophisticated nodes which contain multiple SVG elements, the outcome could be the same. If an example simulation would use 10 SVG elements in just one node, when rendering 100 nodes the outcome would yield a similar outcome as rendering the 6th iteration of the benchmarks with 1000 nodes and 500 links.

Overall all the prototypes perfromed pretty well. As mentioned before, the pure D3 component is the clear winner and best perfroming prototype but that was the expected result, as it was developed to serve as a baseline for the results of the other prototypes. Glancing over the rest of the results, the hybrid almost always comes out ahead of the pure React prototype. The purpose of the study was though, if React can be combined with D3 without losing performance in the browser. The definitive answer to that question has to be no Unfortunately, as there are some performance penalties when combining two full grown libraries. The test results also support the answer to the research question.

When talking about user experience though, the prototypes are pretty much all usable in production projects, as the divergence of the performance numbers mostly stays within the bounds of a smooth animation experience. The benchmark results of the third and fourth iteration are acceptable when looking at the fps and average frame time. When using the force graphs on mobile devices, the performance is worse overall, but due to the fact that even the pure D3 prototypes performed a lot worse on low-end devices, the combination prototypes cannot magically yield a better performance, as they technically not only have to calculate D3 force simulation ticks but also React render cycles. As a consequence the performance is worse than just letting D3 handle the whole simulation on its own. 

